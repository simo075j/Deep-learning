{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks from scratch\n",
    "\n",
    "**- A hand-coded base python and Numpy approach**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple neuron\n",
    "Considering the following basic neuron with 3 inputs:\n",
    "\n",
    "```python\n",
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [0.4, 0.5, 0.6]\n",
    "bias = 0.5\n",
    "```\n",
    "\n",
    "The output $y$ can be calculated by:\n",
    "\n",
    "$$y = X \\cdot w + b$$\n",
    "\n",
    "Where $X$ is the inputs, $w$ is the weights and $b$ is the bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output calculation of the neuron using plain python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0]\n",
    "weights = [0.4, 0.5, 0.6]\n",
    "bias = 0.5\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(np.round(output, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output calculation of the neuron using numpy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array(inputs) # Vectorize the input from above using numpy\n",
    "weights = np.array(weights) # Vectorize the input from above using numpy\n",
    "bias = 0.5\n",
    "\n",
    "output1 = np.dot(inputs,weights)+bias # Using the np.dot function to calculate the dotproduct\n",
    "print(np.round(output1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple fully connected neural network\n",
    "- Creating a simple layer of 3 neurons with 4 inputs in plain python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Context to the approach:</font>\n",
    "What is worth demonstrating here is the ratio between inputs, weights, biases and output.\n",
    "The scenario with 4 inputs and 3 neruons generates 12 unique weight combinations while only having 1 bias per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9 4.4 11.0\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 4.0]\n",
    "\n",
    "w1 = [0.1, 0.2, 0.3, 0,4]\n",
    "w2 = [0.5, 0.6, 0.7, 0,8]\n",
    "w3 = [0.9, 1.0, 1.0, 1.1]\n",
    "\n",
    "b1 = 0.5\n",
    "b2 = 0.6\n",
    "b3 = 0.7\n",
    "\n",
    "output1 = inputs[0]*w1[0] + inputs[1]*w1[1] + inputs[2]*w1[2] + inputs[3]*w1[3] + b1\n",
    "output2 = inputs[0]*w2[0] + inputs[1]*w2[1] + inputs[2]*w2[2] + inputs[3]*w2[3] + b2\n",
    "output3 = inputs[0]*w3[0] + inputs[1]*w3[1] + inputs[2]*w3[2] + inputs[3]*w3[3] + b3\n",
    "\n",
    "print(np.round(output1,1), np.round(output2,1), np.round(output3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**- Now simplifying the same neural network structure with Numpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neuron output in vector format: \n",
      " [[-2.76 -6.85 -1.64]] \n",
      "\n",
      " Neuron 1 output: [-2.76] \n",
      " Neuron 2 output: [-6.85] \n",
      " Neuron 3 output: [-1.64]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1) # setting seed for reproducibility(random numbers in a computer is not actually random)\n",
    "\n",
    "np_input = np.random.uniform(0,9,4) # A 1x4 vector of floats between 0 and 9\n",
    "\n",
    "np_weights = 2 * np.random.random((4,3))-1 # A 4x3 vector of random normalized floats (mean=0 and std=1)\n",
    "\n",
    "np_bias = 2*np.random.random((1,3)) -1 # A 3x1 vector of random normalized floats bewteen 0 and 1 (mean=0 and std=1)\n",
    "\n",
    "output_tot = np.dot(np_input, np_weights) + np_bias # Computing the output of the neurons\n",
    "\n",
    "print(f\" Neuron output in vector format: \\n {output_tot.round(2)} \\n\")\n",
    "\n",
    "output_1 = output_tot[:,0]\n",
    "output_2 = output_tot[:,1]\n",
    "output_3 = output_tot[:,2]\n",
    "\n",
    "print(f\" Neuron 1 output: {output_1.round(2)} \\n Neuron 2 output: {output_2.round(2)} \\n Neuron 3 output: {output_3.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a fully connected (dense) neural network using numpy and classes\n",
    "Using numpy to create a class `LayerDense` and defining the `__init__` method with the following parameters: `self`, `n_inputs` and `n_neurons`.\n",
    "Using `n_neurons` the class will be able to handle any number of neurons in a given layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Context to the approach:</font>\n",
    "*Each of the following code blocks have been indicated with a number (#x) - i will refer to these in this markdown cell to give context in a broader sense.*\n",
    "\n",
    "**#1** In this cell i introduce the first iteration of the layer_dense class, which uses the __init__ method to create a self-referring and dynamic class. The inital class takes a number of inputs and a number of neurons in the layer.\n",
    "\n",
    "**#2** In cell 2 i introduce a forward function which takes an input and calculates an output dependant on the shape of input using the np.dot function + bias\n",
    "\n",
    "**#3** In cell 3 i take the network for a spin for the first time, introducing 2 layers and passing a matrix \"X\" trough the newly defined forward function.\n",
    "\n",
    "**#4** Cell 4 introduced a new class defining a ReLU activation function, which transforms all input < 0 = 0 and keepts everything above in its original state. This can be done in many ways whereas one way is: (x>=0)*x\n",
    "\n",
    "**#5** In this final cell i wrap the forward output in the ReLU function in both layers (I assume that layer 2 is not a final output layer, thus it is fitting to apply a ReLU activation. If it was the final output layer, one should apply a softmax/sigmoid or no activation on the final layer.\n",
    "\n",
    "   **NB:** Best practise would also be to normalize the input \"X\" to a mean=0 and std=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "np.random.seed(1)\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.random.randn(1,n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "np.random.seed(1)\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.random.randn(1,n_neurons)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining two layers, `layer1` and `layer2` and forward propagating `X` over the two layers and showing the final output of `layer2`\n",
    "\n",
    "Input = X\n",
    "```python\n",
    "X = [[ 2.0, 1.0,  3.0,  1.5],\n",
    "     [ 1.0, 5.0, -2.0,  3.0],\n",
    "     [-1.5, 1.5,  3.0, -0.5]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " layer2 output without an activation function: \n",
      " [[-11.50641272  -6.25503609  -7.28963069]\n",
      " [ -9.86446502  -9.84399458 -11.91027699]\n",
      " [  0.28883079  -1.12846935   0.54100381]]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "np.random.seed(1)\n",
    "X = [[ 2.0, 1.0,  3.0,  1.5],\n",
    "     [ 1.0, 5.0, -2.0,  3.0],\n",
    "     [-1.5, 1.5,  3.0, -0.5]]\n",
    "\n",
    "### layer 1 ###\n",
    "layer1 = Layer_Dense(4,3) # The layer takes 4 inputs and passes it trough 3 neurons\n",
    "layer1.forward(X) # Forward the input(X) trough the forward function(without activation)\n",
    "\n",
    "### Layer 2 ###\n",
    "layer2 = Layer_Dense(3,3) # The layer takes 3 inputs and passes it trough 3 neurons\n",
    "layer2.forward(layer1.output) # Passing the output of layer 1 trough layer 2 (without activation)\n",
    "\n",
    "print(f\" layer2 output without an activation function: \\n {layer2.output}\") # Without activiation (layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs) # seeting all negative values = 0 else \"do nothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.71368463 0.         0.        ]\n",
      " [0.         9.23996499 0.        ]\n",
      " [0.9907323  0.         0.46090667]]\n",
      " layer2 output with a ReLU activation function: \n",
      " [[0.         0.         0.        ]\n",
      " [1.29090949 4.70146431 0.        ]\n",
      " [0.33876894 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "np.random.seed(1)\n",
    "### layer 1 ###\n",
    "layer1 = Layer_Dense(4,3) # The layer takes 4 inputs and passes it trough 3 neurons\n",
    "activation1 = Activation_ReLU() \n",
    "layer1.forward(X) # Forward the input(X) trough the forward function(without activation)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "print(activation1.output) # Print output with activation(layer1)\n",
    "\n",
    "### Layer 2 ###\n",
    "layer2 = Layer_Dense(3,3) # The layer takes 3 inputs and passes it trough 3 neuron\n",
    "activation2 = Activation_ReLU()\n",
    "layer2.forward(activation1.output) # Passing the (activation)output of layer 1 trough layer 2 (without activation)\n",
    "activation2.forward(layer2.output) # Wrapping the output in the activation function to get the final output of the last layer\n",
    "\n",
    "print(f\" layer2 output with a ReLU activation function: \\n {activation2.output}\") # With activation (layer1 & layer2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
